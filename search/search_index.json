{
    "docs": [
        {
            "location": "/", 
            "text": "M3DB, a distributed time series database\n\n\nThese documents provide documentation on the overview and operational guidelines for M3DB.\n\n\nAbout\n\n\nM3DB, inspired by \nGorilla\n and \nInfluxDB\n, is a distributed time series database released as open source by \nUber Technologies\n. It can be used for storing realtime metrics at long retention:\n\n\n\n\nDistributed hybrid in-memory and disk time series storage\n\n\nCluster management built on top of \netcd\n\n\nM3TSZ float64 compression inspired by Gorilla TSZ compression\n\n\nArbitrary time precision down to nanosecond precision with the ability to switch precision with any write\n\n\nConfigurable out of order writes\n\n\n\n\nCurrent Limitations\n\n\nDue to the nature of the requirements for the project, which are primarily to reduce the cost of ingesting and storing billions of timeseries and provide fast scalable reads, there are a few limitations currently that make M3DB not suitable for use as a general purpose time series database.\n\n\nThe project has aimed to avoid compactions when at all possible, currently the only compactions that occur are in memory for the current mutable compressed time series block (default configured at 2 hours). As such out of order writes are limited to the size of a single compressed time series block, and consequently means backfilling large amounts of data is not currently possible.\n\n\nThe project has also has optimized for the storage and retrieval of float64 values, as such there is no way to use it as a general time series database of arbitrary data structures just yet.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#m3db-a-distributed-time-series-database", 
            "text": "These documents provide documentation on the overview and operational guidelines for M3DB.", 
            "title": "M3DB, a distributed time series database"
        }, 
        {
            "location": "/#about", 
            "text": "M3DB, inspired by  Gorilla  and  InfluxDB , is a distributed time series database released as open source by  Uber Technologies . It can be used for storing realtime metrics at long retention:   Distributed hybrid in-memory and disk time series storage  Cluster management built on top of  etcd  M3TSZ float64 compression inspired by Gorilla TSZ compression  Arbitrary time precision down to nanosecond precision with the ability to switch precision with any write  Configurable out of order writes", 
            "title": "About"
        }, 
        {
            "location": "/#current-limitations", 
            "text": "Due to the nature of the requirements for the project, which are primarily to reduce the cost of ingesting and storing billions of timeseries and provide fast scalable reads, there are a few limitations currently that make M3DB not suitable for use as a general purpose time series database.  The project has aimed to avoid compactions when at all possible, currently the only compactions that occur are in memory for the current mutable compressed time series block (default configured at 2 hours). As such out of order writes are limited to the size of a single compressed time series block, and consequently means backfilling large amounts of data is not currently possible.  The project has also has optimized for the storage and retrieval of float64 values, as such there is no way to use it as a general time series database of arbitrary data structures just yet.", 
            "title": "Current Limitations"
        }, 
        {
            "location": "/architecture/", 
            "text": "Architecture\n\n\nOverview\n\n\nM3DB is written entirely in Go and has a single dependency of etcd which is used for cluster membership and topology definition. In the near future we plan to also support the concept of seed nodes that will run an embedded etcd server, as etcd can be run in an embedded fashion reasonably simply and statically compiled into the M3DB binary itself.\n\n\nHigh Level Goals\n\n\nSome of the high level goals for the project are defined as:\n\n\n\n\n\n\nMonitoring support:\n M3DB was primarily developed for collecting a high volume of monitoring time series data, distributing the storage in a horizontally scalable manner and most efficiently leveraging the hardware.  As such time series that are not read frequently are not kept in memory.\n\n\n\n\n\n\nHighly configurable:\n Provide a high level of configuration to support a wide set of use cases and runtime environments.\n\n\n\n\n\n\nVariable durability:\n Providing variable durability guarentees for the write and read side of storing time series data enables a wider variety of applications to use M3DB. This is why replication is primarily synchronous and is provided with configurable consistency levels, to enable consistent writes and reads. It must be possible to use M3DB with strong guarentees that data was replicated to a quorum of nodes and that the data was durable if desired.", 
            "title": "Overview"
        }, 
        {
            "location": "/architecture/#architecture", 
            "text": "", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/#overview", 
            "text": "M3DB is written entirely in Go and has a single dependency of etcd which is used for cluster membership and topology definition. In the near future we plan to also support the concept of seed nodes that will run an embedded etcd server, as etcd can be run in an embedded fashion reasonably simply and statically compiled into the M3DB binary itself.", 
            "title": "Overview"
        }, 
        {
            "location": "/architecture/#high-level-goals", 
            "text": "Some of the high level goals for the project are defined as:    Monitoring support:  M3DB was primarily developed for collecting a high volume of monitoring time series data, distributing the storage in a horizontally scalable manner and most efficiently leveraging the hardware.  As such time series that are not read frequently are not kept in memory.    Highly configurable:  Provide a high level of configuration to support a wide set of use cases and runtime environments.    Variable durability:  Providing variable durability guarentees for the write and read side of storing time series data enables a wider variety of applications to use M3DB. This is why replication is primarily synchronous and is provided with configurable consistency levels, to enable consistent writes and reads. It must be possible to use M3DB with strong guarentees that data was replicated to a quorum of nodes and that the data was durable if desired.", 
            "title": "High Level Goals"
        }, 
        {
            "location": "/architecture/sharding/", 
            "text": "Sharding\n\n\nTimeseries keys are hashed to a fixed set of virtual shards. Virtual shards are then assigned to physical nodes. M3DB can be configured to use any hashing function and a configured number of shards. By default \nmurmur3\n is used as the hashing function and 4096 virtual shards are configured.\n\n\nReplication\n\n\nLogical shards are placed on per virtual shard per replica with configurable isolation (zone aware, rack aware, etc). For instance, when using rack aware isolation, the set of datacenter racks that locate a replica\u2019s data are distinct to the racks that locate all other replica\u2019s data.\n\n\nReplication is synchronization during a write and depending on the consistency level configured will notify the client on whether a write succeeded or failed with respect to the consistency level and replication achieved.\n\n\nReplica\n\n\nEach replica has its own assignment of a single logical shard per virtual shard.\n\n\nConceptually it can be defined as:\n\n\nReplica {\n  id uint32\n  shards []Shard\n}\n\n\n\n\nShard state\n\n\nEach shard can be conceptually defined as:\n\n\nShard {\n  id uint32\n  assignments []ShardAssignment\n}\n\nShardAssignment {\n  host Host\n  state ShardState\n}\n\nenum ShardState {\n  INITIALIZING,\n  AVAILABLE,\n  LEAVING\n}\n\n\n\n\nShard assignment\n\n\nThe assignment of shards is stored in etcd. When adding, removing or replacing a node shard goal states are assigned for each shard assigned.\n\n\nFor a write to appear as successful for a given replica it must succeed against all assigned hosts for that shard.  That means if there is a given shard with a host assigned as \nLEAVING\n and another host assigned as \nINITIALIZING\n for a given replica writes to both these hosts must appear as successful to return success for a write to that given replica.  Currently however only \nAVAILABLE\n shards count towards consistency, the work to group the \nLEAVING\n and \nINITIALIZING\n shards together when calculating a write success/error is not complete, see \nissue 417\n.\n\n\nIt is up to the nodes themselves to bootstrap shards when the assignment of new shards to it are discovered in the \nINITIALIZING\n state and to transition the state to \nAVAILABLE\n once bootstrapped by calling the cluster management APIs when done.  Using a compare and set this atomically removes the \nLEAVING\n shard still assigned to the node that previously owned it and transitions the shard state on the new node from \nINITIALIZING\n state to \nAVAILABLE\n.\n\n\nNodes will not start serving reads for the new shard until it is \nAVAILABLE\n, meaning not until they have bootstrapped data for those shards.\n\n\nCluster operations\n\n\nNode add\n\n\nWhen a node is added to the cluster it is assigned shards that relieves load fairly from the existing nodes.  The shards assigned to the new node will become \nINITIALIZING\n, the nodes then discover they need to be bootstrapped and will begin bootstrapping the data using all replicas available.  The shards that will be removed from the existing nodes are marked as \nLEAVING\n.\n\n\nNode down\n\n\nA node needs to be explicitly taken out of the cluster.  If a node goes down and is unavailable the clients performing reads will be served an error from the replica for the shard range that the node owns.  During this time it will rely on reads from other replicas to continue uninterrupted operation.\n\n\nNode remove\n\n\nWhen a node is removed the shards it owns are assigned to existing nodes in the cluster.  Remaining servers discover they are now in possession of shards that are \nINITIALIZING\n and need to be bootstrapped and will begin bootstrapping the data using all replicas available.", 
            "title": "Sharding and Replication"
        }, 
        {
            "location": "/architecture/sharding/#sharding", 
            "text": "Timeseries keys are hashed to a fixed set of virtual shards. Virtual shards are then assigned to physical nodes. M3DB can be configured to use any hashing function and a configured number of shards. By default  murmur3  is used as the hashing function and 4096 virtual shards are configured.", 
            "title": "Sharding"
        }, 
        {
            "location": "/architecture/sharding/#replication", 
            "text": "Logical shards are placed on per virtual shard per replica with configurable isolation (zone aware, rack aware, etc). For instance, when using rack aware isolation, the set of datacenter racks that locate a replica\u2019s data are distinct to the racks that locate all other replica\u2019s data.  Replication is synchronization during a write and depending on the consistency level configured will notify the client on whether a write succeeded or failed with respect to the consistency level and replication achieved.", 
            "title": "Replication"
        }, 
        {
            "location": "/architecture/sharding/#replica", 
            "text": "Each replica has its own assignment of a single logical shard per virtual shard.  Conceptually it can be defined as:  Replica {\n  id uint32\n  shards []Shard\n}", 
            "title": "Replica"
        }, 
        {
            "location": "/architecture/sharding/#shard-state", 
            "text": "Each shard can be conceptually defined as:  Shard {\n  id uint32\n  assignments []ShardAssignment\n}\n\nShardAssignment {\n  host Host\n  state ShardState\n}\n\nenum ShardState {\n  INITIALIZING,\n  AVAILABLE,\n  LEAVING\n}", 
            "title": "Shard state"
        }, 
        {
            "location": "/architecture/sharding/#shard-assignment", 
            "text": "The assignment of shards is stored in etcd. When adding, removing or replacing a node shard goal states are assigned for each shard assigned.  For a write to appear as successful for a given replica it must succeed against all assigned hosts for that shard.  That means if there is a given shard with a host assigned as  LEAVING  and another host assigned as  INITIALIZING  for a given replica writes to both these hosts must appear as successful to return success for a write to that given replica.  Currently however only  AVAILABLE  shards count towards consistency, the work to group the  LEAVING  and  INITIALIZING  shards together when calculating a write success/error is not complete, see  issue 417 .  It is up to the nodes themselves to bootstrap shards when the assignment of new shards to it are discovered in the  INITIALIZING  state and to transition the state to  AVAILABLE  once bootstrapped by calling the cluster management APIs when done.  Using a compare and set this atomically removes the  LEAVING  shard still assigned to the node that previously owned it and transitions the shard state on the new node from  INITIALIZING  state to  AVAILABLE .  Nodes will not start serving reads for the new shard until it is  AVAILABLE , meaning not until they have bootstrapped data for those shards.", 
            "title": "Shard assignment"
        }, 
        {
            "location": "/architecture/sharding/#cluster-operations", 
            "text": "", 
            "title": "Cluster operations"
        }, 
        {
            "location": "/architecture/sharding/#node-add", 
            "text": "When a node is added to the cluster it is assigned shards that relieves load fairly from the existing nodes.  The shards assigned to the new node will become  INITIALIZING , the nodes then discover they need to be bootstrapped and will begin bootstrapping the data using all replicas available.  The shards that will be removed from the existing nodes are marked as  LEAVING .", 
            "title": "Node add"
        }, 
        {
            "location": "/architecture/sharding/#node-down", 
            "text": "A node needs to be explicitly taken out of the cluster.  If a node goes down and is unavailable the clients performing reads will be served an error from the replica for the shard range that the node owns.  During this time it will rely on reads from other replicas to continue uninterrupted operation.", 
            "title": "Node down"
        }, 
        {
            "location": "/architecture/sharding/#node-remove", 
            "text": "When a node is removed the shards it owns are assigned to existing nodes in the cluster.  Remaining servers discover they are now in possession of shards that are  INITIALIZING  and need to be bootstrapped and will begin bootstrapping the data using all replicas available.", 
            "title": "Node remove"
        }, 
        {
            "location": "/architecture/consistencylevels/", 
            "text": "Consistency Levels\n\n\nM3DB provides variable consistency levels for read and write operations, as well as cluster connection operations.\n\n\nWrite consistency levels\n\n\n\n\n\n\nOne:\n Corresponds to a single node succeeding for an operation to succeed.\n\n\n\n\n\n\nMajority:\n Corresponds to the majority of nodes succeeding for an operation to succeed.\n\n\n\n\n\n\nAll:\n Corresponds to all nodes succeeding for an operation to succeed.\n\n\n\n\n\n\nRead consistency levels\n\n\n\n\n\n\nOne\n: Corresponds to reading from a single node to designate success.\n\n\n\n\n\n\nUnstrictMajority\n: Corresponds to reading from the majority of nodes but relaxing the constraint when it cannot be met, falling back to returning success when reading from at least a single node after attempting reading from the majority of nodes.\n\n\n\n\n\n\nMajority\n: Corresponds to reading from the majority of nodes to designate success.\n\n\n\n\n\n\nAll:\n Corresponds to reading from all of the nodes to designate success.\n\n\n\n\n\n\nConnect consistency levels\n\n\nConnect consistency levels are used to determine when a client session is deemed as connected before operations can be attempted.\n\n\n\n\n\n\nAny:\n Corresponds to connecting to any number of nodes for all shards, this strategy will attempt to connect to all, then the majority, then one and then fallback to none and as such will always succeed.\n\n\n\n\n\n\nNone:\n Corresponds to connecting to no nodes for all shards and as such will always succeed.\n\n\n\n\n\n\nOne:\n Corresponds to connecting to a single node for all shards.\n\n\n\n\n\n\nMajority:\n Corresponds to connecting to the majority of nodes for all shards.\n\n\n\n\n\n\nAll:\n Corresponds to connecting to all of the nodes for all shards.", 
            "title": "Consistency Levels"
        }, 
        {
            "location": "/architecture/consistencylevels/#consistency-levels", 
            "text": "M3DB provides variable consistency levels for read and write operations, as well as cluster connection operations.", 
            "title": "Consistency Levels"
        }, 
        {
            "location": "/architecture/consistencylevels/#write-consistency-levels", 
            "text": "One:  Corresponds to a single node succeeding for an operation to succeed.    Majority:  Corresponds to the majority of nodes succeeding for an operation to succeed.    All:  Corresponds to all nodes succeeding for an operation to succeed.", 
            "title": "Write consistency levels"
        }, 
        {
            "location": "/architecture/consistencylevels/#read-consistency-levels", 
            "text": "One : Corresponds to reading from a single node to designate success.    UnstrictMajority : Corresponds to reading from the majority of nodes but relaxing the constraint when it cannot be met, falling back to returning success when reading from at least a single node after attempting reading from the majority of nodes.    Majority : Corresponds to reading from the majority of nodes to designate success.    All:  Corresponds to reading from all of the nodes to designate success.", 
            "title": "Read consistency levels"
        }, 
        {
            "location": "/architecture/consistencylevels/#connect-consistency-levels", 
            "text": "Connect consistency levels are used to determine when a client session is deemed as connected before operations can be attempted.    Any:  Corresponds to connecting to any number of nodes for all shards, this strategy will attempt to connect to all, then the majority, then one and then fallback to none and as such will always succeed.    None:  Corresponds to connecting to no nodes for all shards and as such will always succeed.    One:  Corresponds to connecting to a single node for all shards.    Majority:  Corresponds to connecting to the majority of nodes for all shards.    All:  Corresponds to connecting to all of the nodes for all shards.", 
            "title": "Connect consistency levels"
        }, 
        {
            "location": "/architecture/storage/", 
            "text": "Storage\n\n\nOverview\n\n\nTime series values are stored in compressed streams in filesets, one per shard and block time window size.  They are flushed to disk after a block time window becomes unreachable, that is the end of the time window for that block can no longer be written to.  If a process is killed before it has a chance to flush the data for the current time window to disk it must be restored from the commit log.\n\n\nFilesets\n\n\nA fileset has the following files:\n\n\n\n\nInfo file:\n Stores the block time window start and size and other important metadata about the fileset volume.\n\n\nSummaries file:\n Stores a subset of the index file for purposes of keeping the contents in memory and jumping to section of the index file that within a few pages of linear scanning can find the series that is being looked up.\n\n\nIndex file:\n Stores the series metadata and location of compressed stream in the data file for retrieval.\n\n\nData file:\n Stores the series compressed data streams.\n\n\nBloom filter file:\n Stores a bloom filter bitset of all series contained in this fileset for quick knowledge of whether to attempt retrieving a series for this fileset volume.\n\n\nDigests file:\n Stores the digest checksums of the info file, summaries file, index file, data file and bloom filter file in the fileset volume for integrity verification.\n\n\nCheckpoint file:\n Stores a digest of the digests file and written at the succesful completion of a fileset volume being persisted, allows for quickly checking if a volume was completed.\n\n\n\n\n                                                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502     Index File      \u2502   \n\u2502      Info File      \u2502  \u2502   Summaries File    \u2502     \u2502   (sorted by ID)    \u2502   \n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   (sorted by ID)    \u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \n\u2502- Block Start        \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u250c\u2500\n\u2502- Idx                \u2502   \n\u2502- Block Size         \u2502  \u2502- Idx                \u2502  \u2502  \u2502- ID                 \u2502   \n\u2502- Entries (Num)      \u2502  \u2502- ID                 \u2502  \u2502  \u2502- Size               \u2502   \n\u2502- Major Version      \u2502  \u2502- Index Entry Offset \u251c\u2500\u2500\u2518  \u2502- Checksum           \u2502   \n\u2502- Summaries (Num)    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502- Data Entry Offset  \u251c\u2500\u2500\u2510\n\u2502- BloomFilter (K/M)  \u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                       \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  Bloom Filter File  \u2502  \u2502                            \n\u2502    Digests File     \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502- Bitset             \u2502  \u2502  \u2502      Data File      \u2502   \n\u2502- Info file digest   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \n\u2502- Summaries digest   \u2502                           \u2502  \u2502List of:             \u2502   \n\u2502- Index digest       \u2502                           \u2514\u2500\n\u2502  - Marker (16 bytes)\u2502   \n\u2502- Data digest        \u2502                              \u2502  - ID               \u2502   \n\u2502- Bloom filter digest\u2502                              \u2502  - Data (size bytes)\u2502   \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                        \n\u2502   Checkpoint File   \u2502                                                        \n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                                                        \n\u2502- Digests digest     \u2502                                                        \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", 
            "title": "Storage"
        }, 
        {
            "location": "/architecture/storage/#storage", 
            "text": "", 
            "title": "Storage"
        }, 
        {
            "location": "/architecture/storage/#overview", 
            "text": "Time series values are stored in compressed streams in filesets, one per shard and block time window size.  They are flushed to disk after a block time window becomes unreachable, that is the end of the time window for that block can no longer be written to.  If a process is killed before it has a chance to flush the data for the current time window to disk it must be restored from the commit log.", 
            "title": "Overview"
        }, 
        {
            "location": "/architecture/storage/#filesets", 
            "text": "A fileset has the following files:   Info file:  Stores the block time window start and size and other important metadata about the fileset volume.  Summaries file:  Stores a subset of the index file for purposes of keeping the contents in memory and jumping to section of the index file that within a few pages of linear scanning can find the series that is being looked up.  Index file:  Stores the series metadata and location of compressed stream in the data file for retrieval.  Data file:  Stores the series compressed data streams.  Bloom filter file:  Stores a bloom filter bitset of all series contained in this fileset for quick knowledge of whether to attempt retrieving a series for this fileset volume.  Digests file:  Stores the digest checksums of the info file, summaries file, index file, data file and bloom filter file in the fileset volume for integrity verification.  Checkpoint file:  Stores a digest of the digests file and written at the succesful completion of a fileset volume being persisted, allows for quickly checking if a volume was completed.                                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502     Index File      \u2502   \n\u2502      Info File      \u2502  \u2502   Summaries File    \u2502     \u2502   (sorted by ID)    \u2502   \n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   (sorted by ID)    \u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \n\u2502- Block Start        \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u250c\u2500 \u2502- Idx                \u2502   \n\u2502- Block Size         \u2502  \u2502- Idx                \u2502  \u2502  \u2502- ID                 \u2502   \n\u2502- Entries (Num)      \u2502  \u2502- ID                 \u2502  \u2502  \u2502- Size               \u2502   \n\u2502- Major Version      \u2502  \u2502- Index Entry Offset \u251c\u2500\u2500\u2518  \u2502- Checksum           \u2502   \n\u2502- Summaries (Num)    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502- Data Entry Offset  \u251c\u2500\u2500\u2510\n\u2502- BloomFilter (K/M)  \u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                       \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  Bloom Filter File  \u2502  \u2502                            \n\u2502    Digests File     \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502- Bitset             \u2502  \u2502  \u2502      Data File      \u2502   \n\u2502- Info file digest   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \n\u2502- Summaries digest   \u2502                           \u2502  \u2502List of:             \u2502   \n\u2502- Index digest       \u2502                           \u2514\u2500 \u2502  - Marker (16 bytes)\u2502   \n\u2502- Data digest        \u2502                              \u2502  - ID               \u2502   \n\u2502- Bloom filter digest\u2502                              \u2502  - Data (size bytes)\u2502   \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                        \n\u2502   Checkpoint File   \u2502                                                        \n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                                                        \n\u2502- Digests digest     \u2502                                                        \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", 
            "title": "Filesets"
        }, 
        {
            "location": "/architecture/commitlogs/", 
            "text": "Commit Logs\n\n\nIntegrity Levels\n\n\nThere are two integrity levels available for commit logs:\n\n\n\n\nSynchronous:\n write operations must wait until it has finished writing an entry in the commit log to complete.\n\n\nBehind:\n write operations must finish enqueueing an entry to the commit log write queue to complete.\n\n\n\n\nDepending on the data loss requirements users can choose either integrity level.\n\n\nProperties\n\n\nCommit logs will be stamped by the start time, aligned and rotated by a configured time window size. To restore data for an entire block you will require the commit logs from all time commit logs that overlap the block size with buffer past subtracted from the bootstrap start range and buffer future extended onto the bootstrap end range.\n\n\nStructure\n\n\nCommit logs for a given time window are kept in a single file. An info structure keeping metadata is written to the header of the file and all consequent entries are a repeated log structure, optionally containing metadata describing the series if it's the first time a log entry for a given series appears.\n\n\nThe structures can be conceptually described as:\n\n\nCommitLogInfo {\n  start int64\n  duration int64\n  index int64\n}\n\nCommitLog {\n  created int64\n  index uint64\n  metadata bytes\n  timestamp int64\n  value float64\n  unit uint32\n  annotation bytes\n}\n\nCommitLogMetadata {\n  id bytes\n  namespace bytes\n  shard uint32\n}\n\n\n\n\nGarbage Collected\n\n\nCommit logs are garbage collected after all blocks within the retention period in which data inside the commit logs could be applicable have already been flushed to disk as immutable compressed filesets.", 
            "title": "Commit Logs"
        }, 
        {
            "location": "/architecture/commitlogs/#commit-logs", 
            "text": "", 
            "title": "Commit Logs"
        }, 
        {
            "location": "/architecture/commitlogs/#integrity-levels", 
            "text": "There are two integrity levels available for commit logs:   Synchronous:  write operations must wait until it has finished writing an entry in the commit log to complete.  Behind:  write operations must finish enqueueing an entry to the commit log write queue to complete.   Depending on the data loss requirements users can choose either integrity level.", 
            "title": "Integrity Levels"
        }, 
        {
            "location": "/architecture/commitlogs/#properties", 
            "text": "Commit logs will be stamped by the start time, aligned and rotated by a configured time window size. To restore data for an entire block you will require the commit logs from all time commit logs that overlap the block size with buffer past subtracted from the bootstrap start range and buffer future extended onto the bootstrap end range.", 
            "title": "Properties"
        }, 
        {
            "location": "/architecture/commitlogs/#structure", 
            "text": "Commit logs for a given time window are kept in a single file. An info structure keeping metadata is written to the header of the file and all consequent entries are a repeated log structure, optionally containing metadata describing the series if it's the first time a log entry for a given series appears.  The structures can be conceptually described as:  CommitLogInfo {\n  start int64\n  duration int64\n  index int64\n}\n\nCommitLog {\n  created int64\n  index uint64\n  metadata bytes\n  timestamp int64\n  value float64\n  unit uint32\n  annotation bytes\n}\n\nCommitLogMetadata {\n  id bytes\n  namespace bytes\n  shard uint32\n}", 
            "title": "Structure"
        }, 
        {
            "location": "/architecture/commitlogs/#garbage-collected", 
            "text": "Commit logs are garbage collected after all blocks within the retention period in which data inside the commit logs could be applicable have already been flushed to disk as immutable compressed filesets.", 
            "title": "Garbage Collected"
        }, 
        {
            "location": "/architecture/peer_streaming/", 
            "text": "Peer Streaming\n\n\nClient\n\n\nPeer streaming is managed by the M3DB client.  It fetches all blocks from peers for a specified time range for bootstrapping purposes.  It performs the following three steps:\n1) Fetch all metadata for blocks from all peers who own the specified shard\n2) Compares metadata from different peers and determines the best peer(s) from which to stream the actual data\n3) Streams the block data from peers\n\n\nSteps 1, 2 and 3 all happen conccurently.  As metadata streams in, we begin determining which peer is the best source to stream a given block's data for a given series from, and then we begin streaming data from that peer all while we continue to receive metadata.\n\n\nIn terms of error handling, if an error occurs during the metadata streaming portion, then the client will return an error. However, if something goes wrong during the data streaming portion, it will not return an error, and the function will just return as much data as it can.  This is to combat a disaster scenario where a lot of network or load based errors occur and read availability is desired.  This in the future will be configurable so users can decide on which type of behavior they would prefer.\n\n\nThe diagram below depicts the control flow and concurrency (goroutines and channels) in detail:\n\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502                                               \u2502\n                                 \u2502                                               \u2502\n                                 \u2502         FetchBootstrapBlocksFromPeers         \u2502\n                                 \u2502                                               \u2502\n                                 \u2502                                               \u2502\n                                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                         \u2502\n                                                         \u2502\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502                                                      \u2502\n                              \u25bc                                                      \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502         Main routine          \u2502                      \u2502                               \u2502\n              \u2502                               \u2502       Creates        \u2502      Background routine       \u2502\n              \u2502     1) Create metadataCh      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500(pass \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502                               \u2502\n              \u2502 2) Spin up background routine \u2502     metadataCh)      \u2502                               \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502                                                 For each peer\n                              \u2502                                                      \u2502\n                              \u2502                                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502                                     \u2502                \u2502                 \u2502\n                              \u2502                                     \u2502                \u2502                 \u2502\n                              \u2502                                     \u25bc                \u25bc                 \u25bc\n                              \u2502                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502                                  \u2502       StreamBlocksMetadataFromPeer        \u2502\n                              \u2502                                  \u2502                                           \u2502\n                              \u2502                                  \u2502  Stream paginated blocks metadata from a  \u2502\n                              \u2502                                  \u2502        peer while pageToken != nil        \u2502\n                              \u2502                                  \u2502                                           \u2502\n                              \u2502                                  \u2502 For each blocks metadata --\n put metadata \u2502\n                              \u2502                                  \u2502              into metadataCh              \u2502\n                              \u2502                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502           StreamBlocksFromPeers           \u2502\n        \u2502                                           \u2502                                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 1) Create a background goroutine (details \u2502                                     \u2502   streamAndGroupCollectedBlocksMetadata (injected via    \u2502\n        \u2502               to the right)               \u2502                                     \u2502                streamMetadataFn variable)                \u2502\n        \u2502                                           \u2502                                     \u2502                                                          \u2502\n        \u25022) Create a queue per-peer which each have \u2502                                     \u2502 Loop through the metadataCh aggregating blocks metadata  \u2502\n        \u2502   their own internal goroutine and will   \u2502    Creates (pass metadataCh         \u2502per series/block combination from different peers until we\u2502\n        \u2502   stream blocks back per-series from a    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500and enqueueCh)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   have them from all peers for a series/block metadata   \u2502\n        \u2502              specific peer.               \u2502                                     \u2502   combination and then \nsubmit\n them to the enqueueCh    \u2502\n        \u2502                                           \u2502                                     \u2502                                                          \u2502\n        \u2502  3) Loop through the enqueCh and pick an  \u2502                                     \u2502At the end, flush any remaining series/block combinations \u2502\n        \u2502appropriate peer(s) for each series (based \u2502                                     \u2502(that we received from less than N peers) into the enqueCh\u2502\n        \u2502on whether all the peers have the same data\u2502                                     \u2502                         as well.                         \u2502\n        \u2502 or not) and then put that into the queue  \u2502                                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502for that peer so the data will be streamed \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        For each peer\n                              \u2502\n                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                 \u2502            \u2502             \u2502\n                 \u2502            \u2502             \u2502\n                 \u25bc            \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 newPeerBlocksQueue (processFn = streamBlocksBatchFromPeer)  \u2502\n\u2502                                                             \u2502\n\u2502For each peer we're creating a new peerBlocksQueue which will\u2502\n\u2502     stream data blocks from a specific peer (using the      \u2502\n\u2502   streamBlocksBatchFromPeer function) and add them to the   \u2502\n\u2502                        blocksResult                         \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", 
            "title": "Peer Streaming"
        }, 
        {
            "location": "/architecture/peer_streaming/#peer-streaming", 
            "text": "", 
            "title": "Peer Streaming"
        }, 
        {
            "location": "/architecture/peer_streaming/#client", 
            "text": "Peer streaming is managed by the M3DB client.  It fetches all blocks from peers for a specified time range for bootstrapping purposes.  It performs the following three steps:\n1) Fetch all metadata for blocks from all peers who own the specified shard\n2) Compares metadata from different peers and determines the best peer(s) from which to stream the actual data\n3) Streams the block data from peers  Steps 1, 2 and 3 all happen conccurently.  As metadata streams in, we begin determining which peer is the best source to stream a given block's data for a given series from, and then we begin streaming data from that peer all while we continue to receive metadata.  In terms of error handling, if an error occurs during the metadata streaming portion, then the client will return an error. However, if something goes wrong during the data streaming portion, it will not return an error, and the function will just return as much data as it can.  This is to combat a disaster scenario where a lot of network or load based errors occur and read availability is desired.  This in the future will be configurable so users can decide on which type of behavior they would prefer.  The diagram below depicts the control flow and concurrency (goroutines and channels) in detail:                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502                                               \u2502\n                                 \u2502                                               \u2502\n                                 \u2502         FetchBootstrapBlocksFromPeers         \u2502\n                                 \u2502                                               \u2502\n                                 \u2502                                               \u2502\n                                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                         \u2502\n                                                         \u2502\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502                                                      \u2502\n                              \u25bc                                                      \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502         Main routine          \u2502                      \u2502                               \u2502\n              \u2502                               \u2502       Creates        \u2502      Background routine       \u2502\n              \u2502     1) Create metadataCh      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500(pass \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502                               \u2502\n              \u2502 2) Spin up background routine \u2502     metadataCh)      \u2502                               \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502                                                 For each peer\n                              \u2502                                                      \u2502\n                              \u2502                                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502                                     \u2502                \u2502                 \u2502\n                              \u2502                                     \u2502                \u2502                 \u2502\n                              \u2502                                     \u25bc                \u25bc                 \u25bc\n                              \u2502                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502                                  \u2502       StreamBlocksMetadataFromPeer        \u2502\n                              \u2502                                  \u2502                                           \u2502\n                              \u2502                                  \u2502  Stream paginated blocks metadata from a  \u2502\n                              \u2502                                  \u2502        peer while pageToken != nil        \u2502\n                              \u2502                                  \u2502                                           \u2502\n                              \u2502                                  \u2502 For each blocks metadata --  put metadata \u2502\n                              \u2502                                  \u2502              into metadataCh              \u2502\n                              \u2502                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502           StreamBlocksFromPeers           \u2502\n        \u2502                                           \u2502                                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 1) Create a background goroutine (details \u2502                                     \u2502   streamAndGroupCollectedBlocksMetadata (injected via    \u2502\n        \u2502               to the right)               \u2502                                     \u2502                streamMetadataFn variable)                \u2502\n        \u2502                                           \u2502                                     \u2502                                                          \u2502\n        \u25022) Create a queue per-peer which each have \u2502                                     \u2502 Loop through the metadataCh aggregating blocks metadata  \u2502\n        \u2502   their own internal goroutine and will   \u2502    Creates (pass metadataCh         \u2502per series/block combination from different peers until we\u2502\n        \u2502   stream blocks back per-series from a    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500and enqueueCh)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   have them from all peers for a series/block metadata   \u2502\n        \u2502              specific peer.               \u2502                                     \u2502   combination and then  submit  them to the enqueueCh    \u2502\n        \u2502                                           \u2502                                     \u2502                                                          \u2502\n        \u2502  3) Loop through the enqueCh and pick an  \u2502                                     \u2502At the end, flush any remaining series/block combinations \u2502\n        \u2502appropriate peer(s) for each series (based \u2502                                     \u2502(that we received from less than N peers) into the enqueCh\u2502\n        \u2502on whether all the peers have the same data\u2502                                     \u2502                         as well.                         \u2502\n        \u2502 or not) and then put that into the queue  \u2502                                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502for that peer so the data will be streamed \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        For each peer\n                              \u2502\n                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                 \u2502            \u2502             \u2502\n                 \u2502            \u2502             \u2502\n                 \u25bc            \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 newPeerBlocksQueue (processFn = streamBlocksBatchFromPeer)  \u2502\n\u2502                                                             \u2502\n\u2502For each peer we're creating a new peerBlocksQueue which will\u2502\n\u2502     stream data blocks from a specific peer (using the      \u2502\n\u2502   streamBlocksBatchFromPeer function) and add them to the   \u2502\n\u2502                        blocksResult                         \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", 
            "title": "Client"
        }
    ]
}